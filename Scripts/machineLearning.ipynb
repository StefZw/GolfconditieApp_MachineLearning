{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a machine learning model to classify conditions to a surfers level\n",
    "\n",
    "## Prepare the data\n",
    "For the machine learning model, we will only take one location with four parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, LongType, TimestampType\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Read JSON\").getOrCreate()\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"WaarnemingenLijst\", ArrayType(StructType([\n",
    "        StructField(\"Locatie\", StructType([\n",
    "            StructField(\"Code\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"MetingenLijst\", ArrayType(StructType([\n",
    "            StructField(\"Tijdstip\", TimestampType(), True),\n",
    "            StructField(\"Meetwaarde\", StructType([\n",
    "                StructField(\"Waarde_Numeriek\", DoubleType(), True)\n",
    "            ]), True)\n",
    "        ]), True), True),\n",
    "        StructField(\"AquoMetadata\", StructType([\n",
    "            StructField(\"Parameter_Wat_Omschrijving\", StringType(), True),\n",
    "            StructField(\"Grootheid\", StructType([\n",
    "                StructField(\"Code\", StringType(), True)\n",
    "            ]), True),\n",
    "        ]), True)\n",
    "    ]), True), True)\n",
    "])\n",
    "\n",
    "#Provide the list of jsons to be read\n",
    "files = [\n",
    "'./../Data/Raw/RWS_SPY_WINDRTG.json',\n",
    "'./../Data/Raw/RWS_SPY_WINDSHD.json',\n",
    "'./../Data/Raw/RWS_SPY_Hm0.json',\n",
    "'./../Data/Raw/RWS_SPY_Tm02.json',\n",
    "]\n",
    "\n",
    "# Read the JSON file into a DataFrame using the schema\n",
    "df = spark.read.json(    \\\n",
    "    path=files,  \\\n",
    "    schema=schema,  \\\n",
    "    multiLine=True \\\n",
    "    )\n",
    "\n",
    "# Select and explode the nested fields to get the desired columns\n",
    "df_exploded = df.select(\n",
    "    explode(\"WaarnemingenLijst\").alias(\"Waarnemingen\")\n",
    ").select(\n",
    "    col(\"Waarnemingen.Locatie.Code\").alias(\"location_code\"),\n",
    "    col(\"Waarnemingen.AquoMetadata.Grootheid.Code\").alias(\"parameter_code\"),\n",
    "    explode(\"Waarnemingen.MetingenLijst\").alias(\"Metingen\")\n",
    ").select(\n",
    "    col(\"location_code\"),\n",
    "    col(\"parameter_code\"),\n",
    "    col(\"Metingen.Tijdstip\").alias(\"date_time\"),\n",
    "    col(\"Metingen.Meetwaarde.Waarde_Numeriek\").alias(\"measurement_value\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "Delete all values higher than 900 from the measurements (errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_final = df_exploded.withColumn(\n",
    "    \"measurement_value\",\n",
    "    when(col(\"measurement_value\") > 900, 0).otherwise(col(\"measurement_value\"))\n",
    ")\n",
    "\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the base dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Create fact table with distinct date_time and location_code\n",
    "dataset_0 = df_final.select(\n",
    "    col(\"date_time\"),\n",
    "    col(\"location_code\"),\n",
    ").distinct().withColumn(\"measurement_id\", monotonically_increasing_id() + 1)\n",
    "\n",
    "# Add measurements for specific measurement types\n",
    "dataset_0 = dataset_0.join(\n",
    "    df_final.filter(col(\"parameter_code\") == \"WINDSHD\").select(\n",
    "        col(\"date_time\"),\n",
    "        col(\"Location_Code\").alias(\"location_code\"),\n",
    "        col(\"measurement_value\").alias(\"windspeed_measurement\")\n",
    "    ),\n",
    "    on=[\"date_time\", \"location_code\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    df_final.filter(col(\"parameter_code\") == \"WINDRTG\").select(\n",
    "        col(\"date_time\"),\n",
    "        col(\"Location_Code\").alias(\"location_code\"),\n",
    "        col(\"measurement_value\").alias(\"winddirection_measurement\")\n",
    "    ),\n",
    "    on=[\"date_time\", \"location_code\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    df_final.filter(col(\"parameter_code\") == \"Hm0\").select(\n",
    "        col(\"date_time\"),\n",
    "        col(\"Location_Code\").alias(\"location_code\"),\n",
    "        col(\"measurement_value\").alias(\"waveheight_measurement\")\n",
    "    ),\n",
    "    on=[\"date_time\", \"location_code\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    df_final.filter(col(\"parameter_code\") == \"Tm02\").select(\n",
    "        col(\"date_time\"),\n",
    "        col(\"Location_Code\").alias(\"location_code\"),\n",
    "        col(\"measurement_value\").alias(\"waveperiod_measurement\")\n",
    "    ),\n",
    "    on=[\"date_time\", \"location_code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "dataset_0.show()\n",
    "print(f\"Number of rows in dataset_0: {dataset_0.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label the data\n",
    "Add the surf level category labels to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "import math\n",
    "import random\n",
    "\n",
    "dataset_classified = dataset_0.withColumn(\n",
    "    \"category\",\n",
    "    when(col(\"waveperiod_measurement\") < 4, \"Choppy\")\n",
    "    # Onshore winds\n",
    "    .when((col(\"winddirection_measurement\") >= 90 + random.uniform(-20,20)) & (col(\"winddirection_measurement\") <= 140) & (col(\"windspeed_measurement\") > 90), \"Danger\")\n",
    "    .when((col(\"waveheight_measurement\") <= 150 + random.uniform(-20,20)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 45) & (col(\"winddirection_measurement\") <= 180 ) , \"Advanced\")\n",
    "    .when((col(\"waveheight_measurement\") <= 100 + random.uniform(-20,20)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 45) & (col(\"winddirection_measurement\") <= 180 ) , \"Intermediate\")\n",
    "    .when((col(\"waveheight_measurement\") <= 50 + random.uniform(-20,0)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 45) & (col(\"winddirection_measurement\") <= 180 ) , \"Beginner\")\n",
    "    # Offshore winds\n",
    "    .when((col(\"waveheight_measurement\") <= 250 + random.uniform(-20,20)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 225) & (col(\"winddirection_measurement\") <= 360 ) , \"Advanced\")\n",
    "    .when((col(\"waveheight_measurement\") <= 200 + random.uniform(-20,20)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 225) & (col(\"winddirection_measurement\") <= 360 ) , \"Intermediate\")\n",
    "    .when((col(\"waveheight_measurement\") <= 150 + random.uniform(-20,0)) & (col(\"windspeed_measurement\") <= 20) & (col(\"winddirection_measurement\") <= 90) & (col(\"winddirection_measurement\") >= 225) & (col(\"winddirection_measurement\") <= 360 ) , \"Beginner\")\n",
    "    .when(col(\"waveheight_measurement\") <= 50 + random.uniform(-20,0), \"Flat\")\n",
    "    # Side shore winds\n",
    "    .when(col(\"waveheight_measurement\") <= 100 + random.uniform(-20,20), \"Beginner\")\n",
    "    .when(col(\"waveheight_measurement\") <= 150 + random.uniform(-20,20), \"Intermediate\")\n",
    "    .when(col(\"waveheight_measurement\") <= 250 + random.uniform(-20,20), \"Advanced\")\n",
    "    .when(col(\"waveheight_measurement\") > 250 + random.uniform(-20,0), \"Danger\")\n",
    "    .otherwise(\"Undefined\")\n",
    ")\n",
    "\n",
    "dataset_classified.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the classified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = dataset_classified.select(\"winddirection_measurement\", \"waveheight_measurement\", \"category\").toPandas()\n",
    "\n",
    "# Define colors for each category\n",
    "colors = {\n",
    "    'Danger': 'red',\n",
    "    'Advanced': 'blue',\n",
    "    'Intermediate': 'green',\n",
    "    'Flat': 'purple',\n",
    "    'Choppy': 'orange',\n",
    "    'Beginner': 'brown'\n",
    "}\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category, color in colors.items():\n",
    "    subset = pandas_df[pandas_df['category'] == category]\n",
    "    plt.scatter(subset['winddirection_measurement'], subset['waveheight_measurement'], alpha=0.5, label=category, color=color)\n",
    "\n",
    "plt.xlabel('Wind Direction Measurement')\n",
    "plt.ylabel('Wave Height Measurement')\n",
    "plt.title('Wave Height Measurement by Wind Direction and Category')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean columns\n",
    "Keep only the relevant columns for the training and validation of the machinelearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_classified = dataset_classified.select(\n",
    "    col(\"windspeed_measurement\"),\n",
    "    col(\"winddirection_measurement\"),\n",
    "    col(\"waveheight_measurement\"),\n",
    "    col(\"waveperiod_measurement\"),\n",
    "    col(\"category\")\n",
    ")\n",
    "\n",
    "dataset_classified.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "\n",
    "The dataset is first split into a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset_classified.select(\n",
    "    col(\"windspeed_measurement\"),\n",
    "    col(\"winddirection_measurement\"),\n",
    "    col(\"waveheight_measurement\"),\n",
    "    col(\"waveperiod_measurement\")   \n",
    ").toPandas()\n",
    "\n",
    "y = dataset_classified.select(\"category\").toPandas()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(f\"Number of rows in X_train: {X_train.count()}\")\n",
    "print(f\"Number of rows in X_test: {X_test.count()}\")\n",
    "print(f\"Number of rows in y_train: {y_train.count()}\")\n",
    "print(f\"Number of rows in y_test: {y_test.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the category occurance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = dataset_classified.groupBy('category').count().toPandas()\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "pandas_df = pandas_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pandas_df['category'], pandas_df['count'])\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Category Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can balance the categories in the train dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=15)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create the SVC model\n",
    "svc_model = SVC(kernel='linear', decision_function_shape='ovo', )\n",
    "# Fit the model\n",
    "start_time = time.time()\n",
    "svc_model.fit(X_train_balanced, y_train_balanced)\n",
    "training_time = time.time() - start_time\n",
    "# Predict the categories\n",
    "start_time = time.time()\n",
    "y_pred_svc_0v0 = svc_model.predict(X_test)\n",
    "prediction_time = time.time() - start_time\n",
    "# Print the classification report  \n",
    "print(\"y_pred_svc_0v0 classification_report:\")\n",
    "print(classification_report(y_test, y_pred_svc_0v0))\n",
    "# Print the confusion matrix\n",
    "print(\"y_pred_svc_0v0 confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svc_0v0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory usage\n",
    "mem_usage = memory_usage((svc_model.fit, (X_train, y_train)))\n",
    "\n",
    "print(f\"Training time: {training_time} seconds\")\n",
    "print(f\"Prediction time: {prediction_time} seconds\")\n",
    "print(f\"Memory usage: {max(mem_usage) - min(mem_usage)} MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
